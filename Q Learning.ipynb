{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Reinforcement learning using Q-learning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <center><font color=\"blue\">Content</font></center>\n",
    "[Introduction](#Introduction)\n",
    "\n",
    "[Terminology](#Terminology)\n",
    "\n",
    "[Path problem solution using Q learning](#Path-problem-solution-using-Q-learning)\n",
    "\n",
    "[References](#References:) \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Machine learning` is a subset of artificial intelligence which provides machines the ability to learn automatically and improve from experience without being explicily programmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Reinforcement learning`(RL) is a type of machine learning where an agent learns to behave in a environment by performing actions and seeing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><IMG src=\"https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following parameters are need ti attain a solution\n",
    "- Set of action,A\n",
    "- Set of states,S\n",
    "- Rewards,R\n",
    "- Policy,${\\pi}$\n",
    "- Value,V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Agent`: The RL algorithm that learns from trail and error.\n",
    "\n",
    "`Environment`: The world through which the agent moves.\n",
    "\n",
    "`Action`(A): All the possible steps that the agent can take.\n",
    "\n",
    "`State`(S): Current condition returned by the environment.\n",
    "\n",
    "`Reward`(R): An instant return from the environment to appraise the last action.\n",
    "\n",
    "`Policy`(${\\pi}$): The approach that the agent uses to determine the next action based on the current state.\n",
    "\n",
    "`Value` (V): The expected long-term return with discount, as opposed to the short-term reward R.\n",
    "\n",
    "`Action-value`(Q): This is similar to value expected, it takes an extra parameter, the current action(A).\n",
    "\n",
    "`Exploitation`: It is about using the already known exploited information to heighten the reward.\n",
    "\n",
    "`Exploration`: It is about exploring and capturing more information about an environment.\n",
    "\n",
    "`Markov Decision process`: The mathematical approach for mapping a solution in RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path problem solution using Q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem**: Example describes an agent which uses unsupervised training to learn about an unknown environment. Suppose we have 5 rooms in a building connected by doors as shown in the figure below.  We'll number each room 0 through 4.  The outside of the building can be thought of as one big room (5). Find the best path to go out of room 2 to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><IMG src=\"http://mnemstudio.org/ai/path/images/modeling_environment_clip_image002a.gif\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Algorithm**\n",
    "- Set the gamma parameter, and environment rewards in matrix,R\n",
    "- Initialize matrix Q to zero\n",
    "- Select a random initial state\n",
    "- Set initial state = current state\n",
    "- Select one among all possible actions for the current state\n",
    "- Using this possible action, consider going to next state\n",
    "- Get maximum Q value for this next state based on all possible actions\n",
    "- Compute <center>$Q(State, Action)=R(State, Action)+gamma*max[Q(next\\;state, all\\;action)]$</center>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm above is used by the agent to learn from experience.  Each episode is equivalent to one training session.  In each training session, the agent explores the environment (represented by matrix R ), receives the reward (if any) until it reaches the goal state. The purpose of the training is to enhance the 'brain' of our agent, represented by matrix Q.  More training results in a more optimized matrix Q.  In this case, if the matrix Q has been enhanced, instead of exploring around, and going back and forth to the same rooms, the agent will find the fastest route to the goal state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gamma parameter has a range of 0 to 1 (0 <= Gamma < 1).  If Gamma is closer to zero, the agent will tend to consider only immediate rewards.  If Gamma is closer to one, the agent will consider future rewards with greater weight, willing to delay the reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Room is State.\n",
    "\n",
    "- Agent movement from one room to another represents an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reward distribution**\n",
    "\n",
    "We can represent the rooms on a graph, each room as a node, and each door as a link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><IMG src=\"http://mnemstudio.org/ai/path/images/map3a.gif\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we'd like to put an agent in any room, and from that room, go outside the building (this will be our target room). In other words, the goal room is number 5. To set this room as a goal, we'll associate a reward value to each door (i.e. link between nodes). The doors that lead immediately to the goal have an instant reward of 100.  Other doors not directly connected to the target room have zero reward. Because doors are two-way ( 0 leads to 4, and 4 leads back to 0 ), two arrows are assigned to each room. Room 5 loops back to itself with a reward of 100, and all other direct connections to the goal room carry a reward of 100.  In Q-learning, the goal is to reach the state with the highest reward, so that if the agent arrives at the goal, it will remain there forever. This type of goal is called an \"absorbing goal\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"http://mnemstudio.org/ai/path/images/r_matrix1.gif\" width=\"300\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R matrix\n",
    "R=np.matrix([[-1,-1,-1,-1,0,-1],\n",
    "             [-1,-1,-1,0,-1,100],\n",
    "             [-1,-1,-1,0,-1,-1],\n",
    "             [-1,0,0,-1,0,-1],\n",
    "             [-1,0,0,-1,-1,100],\n",
    "             [-1,0,-1,-1,0,100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q matrix\n",
    "Q=np.matrix(np.zeros([6,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma (learning parameter)\n",
    "gamma=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial state (usually to be chosen at random)\n",
    "initial_state=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns all available actions in the state give as an argument\n",
    "def available_actions(state):\n",
    "    current_state_row=R[state,]\n",
    "    av_act=np.where(current_state_row>=0)[1]\n",
    "    return av_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5], dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get available actions in the current state\n",
    "available_act=available_actions(initial_state)\n",
    "available_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function chooses at random which action to be performed within the range of all the available action\n",
    "def sample_next_action(available_action_range):\n",
    "    next_action=int(np.random.choice(available_act,1))\n",
    "    return next_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample next action to be performed\n",
    "action=sample_next_action(available_act)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function updates the Q matrix according to the path selected and the Q learning algorithm\n",
    "def update(current_state,action,gamma):\n",
    "    max_index=np.where(Q[action,]==np.max(Q[action,]))[1]\n",
    "    if max_index.shape[0]>1:\n",
    "        max_index=int(np.random.choice(max_index,size=1))\n",
    "    else:\n",
    "        max_index=int(max_index)\n",
    "    max_value=Q[action,max_index]\n",
    "    # Q learning formula\n",
    "    Q[current_state,action]=R[current_state,action]+gamma*max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Q matrix\n",
    "update(initial_state,action,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  0.,   0.,   0.,   0., 400.,   0.],\n",
       "        [  0.,   0.,   0., 320.,   0., 500.],\n",
       "        [  0.,   0.,   0., 320.,   0.,   0.],\n",
       "        [  0., 400., 256.,   0., 400.,   0.],\n",
       "        [  0., 400., 256.,   0.,   0., 500.],\n",
       "        [  0., 400.,   0.,   0., 400., 500.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "# Train over 10,OOO iterations. [Re-iterate the process above]\n",
    "for i in range(10000):\n",
    "    current_state=np.random.randint(0,int(Q.shape[0]))\n",
    "    available_act=available_actions(current_state)\n",
    "    action=sample_next_action(available_act)\n",
    "    update(current_state,action,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q matrix:\n",
      "[[  0.    0.    0.    0.   80.    0. ]\n",
      " [  0.    0.    0.   64.    0.  100. ]\n",
      " [  0.    0.    0.   64.    0.    0. ]\n",
      " [  0.   80.   51.2   0.   80.    0. ]\n",
      " [  0.   80.   51.2   0.    0.  100. ]\n",
      " [  0.   80.    0.    0.   80.  100. ]]\n"
     ]
    }
   ],
   "source": [
    "# Normalize the \"trained\" Q matrix\n",
    "print(\"Trained Q matrix:\")\n",
    "print(Q/np.max(Q)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected path:\n",
      "[2, 3, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "# Goal state=5\n",
    "# Best sequence path starting from 2->1,3,1,5\n",
    "current_state=2\n",
    "steps=[current_state]\n",
    "while current_state!=5:\n",
    "    next_step_index=np.where(Q[current_state,]==np.max(Q[current_state,]))[1]\n",
    "    if next_step_index.shape[0]>1:\n",
    "        next_step_index=int(np.random.choice(next_step_index,size=1))\n",
    "    else:\n",
    "        next_step_index=int(next_step_index)\n",
    "    steps.append(next_step_index)\n",
    "    current_state=next_step_index\n",
    "# Print selected sequence of steps\n",
    "print(\"Selected path:\")\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><IMG src=\"http://mnemstudio.org/ai/path/images/modeling_environment_clip_image002a.gif\" width=\"300\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "[edureka!][]\n",
    "\n",
    "[Q-learning][]\n",
    "\n",
    "[Q-learning]:http://mnemstudio.org/path-finding-q-learning-tutorial.htm\n",
    "[edureka!]:https://www.youtube.com/watch?v=LzaWrmKL1Z4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
